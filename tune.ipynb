{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GridSearch import GridSearch\n",
    "from keras import backend as K \n",
    "from keras.engine import Layer\n",
    "from keras.layers import Dense, Dropout, Embedding, Flatten, Input, MaxPooling1D\n",
    "from keras.models import Model, load_model, Sequential\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostClassifier, GradientBoostingRegressor\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, MultiTaskElasticNet\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, LeaveOneOut\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVR, LinearSVR\n",
    "from xgboost import XGBRegressor\n",
    "import csv\n",
    "import keras.layers as layers\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import sys\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearch Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridSearch:\n",
    "\n",
    "    def __init__(self, models_dict, params_dict):\n",
    "        # if not set(models_dict.keys()).issubset(set(params_dict.keys())):\n",
    "        #     missing_params = list(set(models.keys()) - set(params_dict.keys()))\n",
    "        #     raise ValueError(\n",
    "        #         \"Some estimators are missing parameters: %s\" % missing_params)\n",
    "        self.models = models_dict\n",
    "        self.params = params_dict\n",
    "        self.keys = models_dict.keys()\n",
    "        #print(self.keys)\n",
    "        self.best_ = {\n",
    "            'estimator': [None],\n",
    "            'params': {},\n",
    "            'y_pred': [],\n",
    "            'r': [],\n",
    "        }\n",
    "\n",
    "    def predict(self):\n",
    "        return self.best_['r']\n",
    "\n",
    "    def tune(self, X_train, y_train, X_test, y_test, **grid_kwargs):\n",
    "        max_r = 0\n",
    "        for key in self.keys:\n",
    "            print(\"\\tRunning GridSearchCV for %s.\" % key)\n",
    "            model = self.models[key]\n",
    "            params = self.params[key]\n",
    "\n",
    "            #Pipeline the estimators\n",
    "            pipeline = Pipeline([\n",
    "                ('clf', model),\n",
    "            ])\n",
    "\n",
    "            gs = GridSearchCV(pipeline, params, **grid_kwargs)\n",
    "            gs.fit(X_train, y_train)\n",
    "\n",
    "            print(\"\\tPredicting for %s.\" % key)\n",
    "            y_pred = gs.predict(X_test)\n",
    "            r = np.corrcoef(y_pred, y_test)[0, 1]\n",
    "            print(params)\n",
    "            print(r)\n",
    "\n",
    "            if (abs(r) > abs(max_r)):\n",
    "                self.best_['estimator'] = model\n",
    "                self.best_['params'] = gs.best_params_\n",
    "                self.best_['r'] = r\n",
    "                self.best_['y_pred'] = y_pred\n",
    "\n",
    "            print(\"Current Best\")\n",
    "            #print(self.best_['params'])\n",
    "            print(self.best_['r'])\n",
    "\n",
    "            print('\\tTuning for %s Done.' % key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################################################\n",
    "# Import the process data from R Studio\n",
    "###############################################################################################################\n",
    "\n",
    "data = pd.read_csv(\"mega_dataset.csv\")\n",
    "\n",
    "train_data = data.loc[\"Dataset\" == \"Train\"]\n",
    "dev_data = data.loc[\"Dataset\" == \"Dev\"]\n",
    "test_data = data.loc[\"Dataset\" == \"Test\"]\n",
    "    \n",
    "\n",
    "# best_hyperparameter_list = tune(train_data_X, train_data_y, dev_data_X, dev_data_y, selected_feature)\n",
    "best_hyperparameters = []\n",
    "clf_dict = {\n",
    "    'neural': KerasRegressor(),\n",
    "    'forest': RandomForestRegressor(),\n",
    "    'ridge': Ridge(),\n",
    "    'elastic': ElasticNet(),\n",
    "}\n",
    "param_list = {\n",
    "    'neural': {\n",
    "        'clf__build_fn': [lambda: ElmoRegressionModel(**model_params)],\n",
    "        'clf__epochs': [10],\n",
    "        'clf__batch_size': [32],\n",
    "        'clf__verbose': [1]\n",
    "    },\n",
    "    'forest': {\n",
    "        'clf__n_estimators': [100],\n",
    "        'clf__criterion': ['mse'],\n",
    "        'clf__max_depth': [None],\n",
    "        'clf__min_samples_split': [2],\n",
    "        'clf__min_samples_leaf': [1],\n",
    "        'clf__min_weight_fraction_leaf': [0.0],\n",
    "        'clf__max_features': ['auto'],\n",
    "        'clf__max_leaf_nodes': [None],\n",
    "        'clf__min_impurity_decrease': [0.0],\n",
    "        'clf__min_impurity_split': [None],\n",
    "        'clf__bootstrap': [True],\n",
    "        'clf__oob_score': [False],\n",
    "        'clf__n_jobs': [None],\n",
    "        'clf__random_state': [None],\n",
    "        'clf__verbose': [0],\n",
    "        'clf__warm_start': [False],\n",
    "        'clf__ccp_alpha': [0.0],\n",
    "        'clf__max_samples': [None]\n",
    "    },\n",
    "    'ridge': {\n",
    "        'clf__alpha': [1.0], \n",
    "        'clf__fit_intercept': [True], \n",
    "        'clf__normalize': [False], \n",
    "        'clf__copy_X': [True],\n",
    "        'clf__max_iter': [None],\n",
    "        'clf__tol': [0.001],\n",
    "        'clf__solver': ['auto'],\n",
    "        'clf__random_state': [None],\n",
    "    },\n",
    "    'elastic': {\n",
    "        'clf__alpha': [1.0],\n",
    "        'clf__l1_ratio': [0.5],\n",
    "        'clf__fit_intercept': [True],\n",
    "        'clf__normalize': [False],\n",
    "        'clf__precompute': [False],\n",
    "        'clf__max_iter': [1000],\n",
    "        'clf__copy_X': [True],\n",
    "        'clf__tol': [0.0001],\n",
    "        'clf__warm_start': [False],\n",
    "        'clf__positive': [False],\n",
    "        'clf__random_state': [None],\n",
    "        'clf__selection': ['cyclic']\n",
    "    },\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in parameter_list.keys():\n",
    "print('Running Parameter TUning for Neural Network')\n",
    "\n",
    "pipe = Pipeline([('clf', clf_dict.get(key))])\n",
    "\n",
    "gs = GridSearchCV(pipe, param_list.get(key), cv=5, n_jobs=1, verbose=1, scoring='r2', return_train_score=False, error_score='raise', iid=True)\n",
    "gs.fit(train_data, dev_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}